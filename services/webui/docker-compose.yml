services:
  llama-cpp:
    # Using the ggml-org ROCm image for AMD GPU support
    image: ghcr.io/ggml-org/llama.cpp:full-rocm
    container_name: llama-cpp
    volumes:
      - ./models:/models
    ports:
      - "10000:10000"
    # We use the 'server' entrypoint to keep the API alive for Open WebUI
    command: >
      --server
      --model /models/Meta-Llama-3.1-8B-Instruct-GGUF
      --port 10000
      --ctx-size 1024
      --n-gpu-layers 40
      --host 0.0.0.0
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    restart: unless-stopped
    networks:
      - ai-net

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "3000:8080"
    volumes:
      - ./open-webui:/app/backend/data
    restart: unless-stopped
    depends_on:
      - llama-cpp
    environment:
      # Open WebUI connects via the OpenAI-compatible API on port 10000
      - OPENAI_API_BASE_URL=http://llama-cpp:10000/v1
      - OPENAI_API_KEY=llama.cpp
    networks:
      - ai-net

networks:
  ai-net:
    driver: bridge
